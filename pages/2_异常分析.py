import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
import streamlit as st
from services.db_service import get_conn
from typing import Union, Optional
from datetime import date
from pathlib import Path

from statsmodels.tsa.statespace.sarimax import SARIMAX
import warnings
warnings.filterwarnings('ignore')


def detect_anomalies_stat(data, target='Stock_quantity', ds='datetime', threshold=5, 
    # weights_list=[1,1,1.1,1,1],
    weights_list=[2,1,1.5,2,3]
    ):
    """
    é˜ˆå€¼               å¼‚å¸¸å®šä¹‰                    é€‚ç”¨åœºæ™¯
    3.0        è¾ƒæ•æ„Ÿï¼Œå¯èƒ½åŒ…å«è½»åº¦å¼‚å¸¸            é¢„è­¦ã€æ—©æœŸå‘ç°
    3.5        æ ‡å‡†æ¨èå€¼ï¼ˆIglewicz & Hoaglinï¼‰   é€šç”¨ã€å¹³è¡¡
    4.0ï½5.0   åªæŠ“æç«¯å¼‚å¸¸å€¼                     å‰”é™¤æ˜æ˜¾é”™è¯¯ã€è„æ•°æ®
    >5.0       æå…¶ä¸¥æ ¼ï¼Œå‡ ä¹åªä¿ç•™å®Œç¾æ•°æ®        é«˜ç²¾åº¦å»ºæ¨¡å‰æ¸…æ´—
    """
    result = data.copy()
    
    # 1. æ£€æŸ¥è¾“å…¥æ•°æ®
    if result.empty:
        raise ValueError("Input data is empty!")
    
    # 2. ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
    result['date'] = pd.to_datetime(result[ds])
    result[target] = pd.to_numeric(result[target], errors='coerce')  # éæ•°å€¼è½¬ä¸ºNaN
    result.dropna(subset=[target], inplace=True)  # åˆ é™¤æ— æ•ˆå€¼
    
    # 3. ç”Ÿæˆå­£åº¦åˆ—
    result['quarter'] = result['date'].dt.to_period('Q')
    quarters = sorted(result['quarter'].unique())
    
    # 4. éå†å­£åº¦æ£€æµ‹å¼‚å¸¸
    for i, quarter in enumerate(quarters):
        if i == 0:
            # é’ˆå¯¹ç¬¬ä¸€ä¸ªå­£åº¦çš„æ•°æ®ï¼Œå°±ç”¨è‡ªèº«æ•°æ®ä½œä¸ºå†å²æ•°æ®
            past_quarters = quarters[0:1]
        else:
            # è·å–è¿‡å»5ä¸ªå­£åº¦çš„æ•°æ®å’Œå½“å‰å­£åº¦å·²æœ‰æ•°æ®
            past_quarters = quarters[max(0, i-5):i+1]

        past_data = result[result['quarter'].isin(past_quarters)]

        q_mean = np.mean(past_data[target])

        q1, q25, q50, q75, q99 = np.quantile(past_data[target], [0.01, 0.25, 0.5, 0.75, 0.99])
        iqr = q75 - q25
        BoxPlot_lower = q25 - iqr * 3
        BoxPlot_upper = q75 + iqr * 3
        
        # è°ƒæ•´æƒé‡è®¡ç®—åŠ æƒmedian
        n_groups = len(past_data.groupby('quarter'))
        weights = np.array(weights_list[-n_groups:])
        weights = weights / weights.sum()
        
        # è®¡ç®—åŠ æƒç»Ÿè®¡é‡
        weighted_values = []
        for (q, group), w in zip(past_data.groupby('quarter')[target], weights):
            n_rep = max(1, int(round(w * 10)))  # æ”¾å¤§10å€åå–æ•´ï¼Œä¿è¯ç²¾åº¦
            weighted_values.extend(np.repeat(group, n_rep))
        
        weighted_values = np.array(weighted_values)

        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        weighted_median = np.median(weighted_values)
        weighted_mad = np.median(np.abs(weighted_values - weighted_median))

        # æ ‡è®°å½“å‰å­£åº¦çš„å¼‚å¸¸
        current_mask = result['quarter'] == quarter
        current_values = result.loc[current_mask, target]
        
        modified_z_scores = 0.6745 * (current_values - weighted_median) / max(weighted_mad, 1e-6)
        delta = (threshold * weighted_mad) / 0.6745
        mZScore_lower = weighted_median - delta
        mZScore_upper = weighted_median + delta
        modified_z_scores = modified_z_scores.fillna(0)
        
        # æ›´æ–°ç»“æœåˆ—
        result.loc[current_mask, f'{target}_mean'] = q50
        result.loc[current_mask, f'{target}_median'] = q_mean
        result.loc[current_mask, f'{target}_q1'] =  q1
        result.loc[current_mask, f'{target}_q25'] = q25
        result.loc[current_mask, f'{target}_q75'] = q75
        result.loc[current_mask, f'{target}_q99'] = q99

        result.loc[current_mask, f'{target}_modified_z_score'] = modified_z_scores
        result.loc[current_mask, f'{target}_mZScore_lower'] = mZScore_lower
        result.loc[current_mask, f'{target}_mZScore_upper'] = mZScore_upper
        result.loc[current_mask, f'{target}_mz_anomaly'] = (np.abs(modified_z_scores) > threshold).astype(bool)

        result.loc[current_mask, f'{target}_BoxPlot_lower'] = BoxPlot_lower
        result.loc[current_mask, f'{target}_BoxPlot_upper'] = BoxPlot_upper
        result.loc[current_mask, f'{target}_box_anomaly'] = ((current_values < BoxPlot_lower) | (current_values > BoxPlot_upper)).astype(bool)

        result.loc[current_mask, f'{target}_trunc_anomaly'] = ((current_values < q1) | (current_values > q99)).astype(bool)

    return result

# Plotly ç»˜å›¾å‡½æ•°
def plot_with_plotly(plot_data, target, method, title):
    fig = go.Figure()
    fig.add_trace(go.Scatter(x=plot_data['date'], y=plot_data[target], name='å®é™…å€¼', mode='lines', line=dict(color='blue')))
    if method == 'ä¿®æ­£Z_score':
        anomaly_col = f'{target}_mz_anomaly'
        fig.add_trace(go.Scatter(x=plot_data['date'], y=plot_data[f'{target}_mZScore_upper'], 
                                 name='ä¸Šç•Œ', mode='lines', line=dict(width=0), showlegend=False))
        fig.add_trace(go.Scatter(x=plot_data['date'], y=plot_data[f'{target}_mZScore_lower'], 
                                 name='ä¸‹ç•Œ', fill='tonexty', mode='lines', line=dict(width=0), 
                                 fillcolor='rgba(0,100,80,0.2)'))
    elif method == "ç®±çº¿å›¾æ³•":
        anomaly_col = f'{target}_box_anomaly'
        fig.add_trace(go.Scatter(x=plot_data['date'], y=plot_data[f'{target}_BoxPlot_upper'], 
                                 name='ä¸Šç•Œ', mode='lines', line=dict(width=0), showlegend=False))
        fig.add_trace(go.Scatter(x=plot_data['date'], y=plot_data[f'{target}_BoxPlot_lower'], 
                                 name='ä¸‹ç•Œ', fill='tonexty', mode='lines', line=dict(width=0), 
                                 fillcolor='rgba(0,100,80,0.2)'))
    elif method == "é«˜åˆ†ä½æˆªæ–­æ³•":
        anomaly_col = f'{target}_trunc_anomaly'
        fig.add_trace(go.Scatter(x=plot_data['date'], y=plot_data[f'{target}_q99'], 
                                 name='ä¸Šç•Œ', mode='lines', line=dict(width=0), showlegend=False))
        fig.add_trace(go.Scatter(x=plot_data['date'], y=plot_data[f'{target}_q1'], 
                                 name='ä¸‹ç•Œ', fill='tonexty', mode='lines', line=dict(width=0), 
                                 fillcolor='rgba(0,100,80,0.2)'))
    
    anomalies = plot_data[plot_data[anomaly_col]]
    n_anomalies = plot_data[anomaly_col].sum()
    title = f"{title}    |    æ£€æµ‹åˆ° {n_anomalies} ä¸ªå¼‚å¸¸ç‚¹ï¼"

    fig.add_trace(go.Scatter(x=anomalies['date'], y=anomalies[target], name='å¼‚å¸¸ç‚¹', mode='markers', 
                             marker=dict(color='red', size=8)))
    fig.update_layout(title=title, xaxis_title='æ—¥æœŸ', yaxis_title=target, legend_title='å›¾ä¾‹', 
                      hovermode='x unified', template='plotly_white',
                      xaxis=dict(rangeslider=dict(visible=True), type="date")
                      )
    return fig

def get_parameters(key_prefix="", target_options=None, dims_options=None):
    """
    è·å–ç”¨æˆ·è¾“å…¥çš„å‚æ•°ï¼Œå¹¶ä¸ºæ¯ä¸ªç»„ä»¶æŒ‡å®šå”¯ä¸€çš„ keyã€‚
    """
    st.write("è¯·è¾“å…¥å‚æ•°è¿›è¡Œåˆ†æï¼š")
    # === ç¬¬ä¸€è¡Œï¼šå¼€å§‹æ—¥æœŸ + ç»“æŸæ—¥æœŸï¼ˆ2åˆ—ï¼‰===
    col1, col2 = st.columns(2)
    with col1:
        start_date = st.date_input("å¼€å§‹æ—¥æœŸ", value=pd.to_datetime("2023-01-01"), 
                                   key=f"{key_prefix}_start_date")
    with col2:
        end_date = st.date_input("ç»“æŸæ—¥æœŸ", value=pd.to_datetime("2025-02-28"), 
                                 key=f"{key_prefix}_end_date")

    # === ç¬¬äºŒè¡Œï¼šåˆ†æçº§åˆ« + ç›®æ ‡å˜é‡ + å¼‚å¸¸æ£€æµ‹æ–¹æ³•ï¼ˆ3åˆ—ï¼‰===
    col1, col2, col3 = st.columns(3)
    with col1:
        target = st.selectbox("é€‰æ‹©ç›®æ ‡å˜é‡", target_options, key=f"{key_prefix}_target")
    with col2:
        analysis_level = st.selectbox("é€‰æ‹©åˆ†æçº§åˆ«", dims_options, key=f"{key_prefix}_analysis_level")
    with col3:
        method = st.selectbox("é€‰æ‹©å¼‚å¸¸æ£€æµ‹æ–¹æ³•", ["ä¿®æ­£Z_score", "ç®±çº¿å›¾æ³•", "é«˜åˆ†ä½æˆªæ–­æ³•"], 
                              key=f"{key_prefix}_method")
    
    return start_date, end_date, analysis_level, target, method

def get_k_by_v(dict, value, default=None):
    """è¿”å›å­—å…¸ä¸­ç¬¬ä¸€ä¸ªåŒ¹é…å€¼çš„é”®ï¼Œæœªæ‰¾åˆ°è¿”å› default"""
    return next((k for k, v in dict.items() if v == value), default)

# æå–æ•°æ®åˆ†æéƒ¨åˆ†
def analyze_data(data, analysis_level, target, config, start_date, end_date, key_prefix=""):
    """
    æ ¹æ®åˆ†æçº§åˆ«è¿‡æ»¤æ•°æ®ï¼Œå¹¶ç”Ÿæˆæ ‡é¢˜ã€‚
    """
    ds, l1, l2, l3 = get_k_by_v(config, "datetime"), get_k_by_v(config, "level-1"), get_k_by_v(config, "level-2"), get_k_by_v(config, "level-3")

    data[ds] = pd.to_datetime(data[ds])

    if analysis_level == l3:
        l3_v = st.selectbox(f"é€‰æ‹© {l3}", data[l3].unique(), 
            key=f"{key_prefix}_sku_select"  # æ·»åŠ å”¯ä¸€ key
        )
        filtered_data = data[data[l3] == l3_v]
        filtered_data = detect_anomalies_stat(filtered_data, target=target, ds=ds)
        title = f"{l3}: {l3_v}"
    elif analysis_level == l1:
        l1_v = st.selectbox(f"é€‰æ‹©{l1}", data[l1].unique(), 
            key=f"{key_prefix}_category_select"  # æ·»åŠ å”¯ä¸€ key
        )
        filtered_data = data[data[l1] == l1_v].groupby(ds).agg({
            target: 'sum'
        }).reset_index()
        filtered_data = detect_anomalies_stat(filtered_data, target=target, ds=ds)
        title = f"{l1}: {l1_v}"
    else:
        l2_v = st.selectbox(f"é€‰æ‹©{l2}", data[l2].unique(), 
            key=f"{key_prefix}_subcategory_select"  # æ·»åŠ å”¯ä¸€ key
        )
        filtered_data = data[data[l2] == l2_v].groupby(ds).agg({
            target: 'sum'
        }).reset_index()
        filtered_data = detect_anomalies_stat(filtered_data, target=target, ds=ds)
        title = f"{l2}: {l2_v}"

    st.write("è¯·åœ¨ä¸Šéƒ¨é€‰æ‹©å‚æ•°å¹¶ç‚¹å‡» 'è¿è¡Œåˆ†æ' æŒ‰é’®")

    # è¿‡æ»¤æ—¥æœŸèŒƒå›´
    filtered_data = filtered_data[
        (filtered_data[ds] >= pd.to_datetime(start_date)) &
        (filtered_data[ds] <= pd.to_datetime(end_date))
    ]
    return filtered_data, title

def prepare_ts_data(data, sku=None, category=None, subcategory=None, target="sales_volume", 
                     start_date=None, end_date=None):
    """
    ä½¿ç”¨SARIMAXæ¨¡å‹æ£€æµ‹å¼‚å¸¸å¹¶è¿”å›é¢„æµ‹ç»“æœ
    è¿”å›ï¼šDataFrameï¼ˆå«é¢„æµ‹æ•°æ®ä¸å¼‚å¸¸æ ‡è®°ï¼‰åŠå›¾è¡¨æ ‡é¢˜
    """
    if sku is not None:
        full_data = data[data['sku'] == sku][['date', target]].rename(columns={'date': 'ds', target: 'y'})
        title = f'{target} for SKU: {sku}'
    elif category is not None:
        full_data = data[data['category'] == category].groupby('date')[target].sum().reset_index().rename(
            columns={'date': 'ds', target: 'y'})
        title = f'{target} for Category: {category}'
    elif subcategory is not None:
        full_data = data[data['subcategory'] == subcategory].groupby('date')[target].sum().reset_index().rename(
            columns={'date': 'ds', target: 'y'})
        title = f'{target} for Subcategory: {subcategory}'
    else:
        raise ValueError("å¿…é¡»æŒ‡å®š skuã€category æˆ– subcategory")

    # å°†dsåˆ—è½¬æ¢ä¸ºdatetime
    full_data['ds'] = pd.to_datetime(full_data['ds'])

    # æ—¶é—´èŒƒå›´è¿‡æ»¤
    if start_date is not None:
        start_date = pd.to_datetime(start_date)
        result_df = full_data[full_data['ds'] >= start_date]
    if end_date is not None:
        end_date = pd.to_datetime(end_date)
        result_df = result_df[result_df['ds'] <= end_date]
    
    return result_df, title

# ARIMAX
def detect_anomalies_sarimax(data, confidence_coef=0.95):
    """
    ä½¿ç”¨SARIMAXæ¨¡å‹æ£€æµ‹å¼‚å¸¸å¹¶è¿”å›é¢„æµ‹ç»“æœ
    è¿”å›ï¼šDataFrameï¼ˆå«é¢„æµ‹æ•°æ®ä¸å¼‚å¸¸æ ‡è®°ï¼‰åŠå›¾è¡¨æ ‡é¢˜
    """

    # å°†dsè®¾ç½®ä¸ºç´¢å¼•
    data.set_index('ds', inplace=True)
    
    # é‡æ–°é‡‡æ ·ä»¥ç¡®ä¿æ—¶é—´åºåˆ—è¿ç»­ï¼ˆæŒ‰å¤©ï¼‰
    full_data = data.asfreq('D')
    
    # å¡«å……ç¼ºå¤±å€¼
    full_data['y'] = full_data['y'].fillna(method='ffill').fillna(method='bfill')
    
    # ç¡®å®šSARIMAXå‚æ•°
    # å­£èŠ‚å‘¨æœŸè®¾ä¸º365å¤©ï¼ˆå¹´å­£èŠ‚æ€§ï¼‰
    seasonal_order = (1, 1, 1, 7)  # P, D, Q, s (å‘¨å­£èŠ‚æ€§)
    
    # æ‹ŸåˆSARIMAXæ¨¡å‹
    # ä½¿ç”¨å·®åˆ†æ¥å¤„ç†è¶‹åŠ¿
    model = SARIMAX(full_data['y'], 
                    order=(1, 1, 1),  # p, d, q
                    seasonal_order=seasonal_order,
                    enforce_stationarity=False,
                    enforce_invertibility=False)
    
    fitted_model = model.fit(disp=False)
    
    # é¢„æµ‹æœªæ¥30å¤©
    forecast_steps = 30
    forecast_result = fitted_model.get_forecast(steps=forecast_steps)
    forecast_mean = forecast_result.predicted_mean
    forecast_conf_int = forecast_result.conf_int(alpha=1-confidence_coef)
    
    # é¢„æµ‹å†å²æ•°æ®ä»¥è·å–ç½®ä¿¡åŒºé—´
    history_forecast = fitted_model.get_prediction(start=0, end=len(full_data)-1)
    history_mean = history_forecast.predicted_mean
    history_conf_int = history_forecast.conf_int(alpha=1-confidence_coef)
    
    # åˆ›å»ºç»“æœDataFrame
    # é¦–å…ˆä¸ºå†å²æ•°æ®åˆ›å»ºDataFrame
    history_df = pd.DataFrame({
        'ds': full_data.index,
        'y': full_data['y'],
        'yhat': history_mean.values,
        'yhat_lower': history_conf_int.iloc[:, 0].values,
        'yhat_upper': history_conf_int.iloc[:, 1].values
    })
    
    # åˆ›å»ºæœªæ¥é¢„æµ‹çš„DataFrame
    last_date = full_data.index[-1]
    future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=forecast_steps, freq='D')
    future_df = pd.DataFrame({
        'ds': future_dates,
        'y': [np.nan] * forecast_steps,  # æœªæ¥çš„çœŸå®å€¼æœªçŸ¥
        'yhat': forecast_mean.values,
        'yhat_lower': forecast_conf_int.iloc[:, 0].values,
        'yhat_upper': forecast_conf_int.iloc[:, 1].values
    })
    
    # åˆå¹¶å†å²å’Œæœªæ¥æ•°æ®
    result_df = pd.concat([history_df, future_df], ignore_index=True)
    
    # æ·»åŠ å¼‚å¸¸æ ‡è®°
    result_df['is_anomaly'] = False
    # åªå¯¹æœ‰å®é™…å€¼çš„æ•°æ®ç‚¹è¿›è¡Œå¼‚å¸¸æ£€æµ‹
    mask = ~result_df['y'].isna()
    result_df.loc[mask, 'is_anomaly'] = (
        (result_df.loc[mask, 'y'] < result_df.loc[mask, 'yhat_lower']) | 
        (result_df.loc[mask, 'y'] > result_df.loc[mask, 'yhat_upper'])
    )
    
    # é‡ç½®ç´¢å¼•
    result_df.reset_index(drop=True, inplace=True)
    
    return result_df

def plot_with_plotly2(plot_data, title, target, confidence_coef):
    """
    ä½¿ç”¨ Plotly ç»˜åˆ¶äº¤äº’å¼æ—¶é—´åºåˆ—å›¾
    è¿”å› Plotly Figure å¯¹è±¡
    """
    fig = go.Figure()

    # å®é™…å€¼
    fig.add_trace(go.Scatter(x=plot_data['ds'], y=plot_data['y'], name='å®é™…å€¼', mode='lines+markers', line=dict(color='blue')))
    # é¢„æµ‹å€¼
    fig.add_trace(go.Scatter(x=plot_data['ds'], y=plot_data['yhat'], name='é¢„æµ‹å€¼', mode='lines', line=dict(color='orange')))
    # ç½®ä¿¡åŒºé—´
    fig.add_trace(go.Scatter(x=plot_data['ds'], y=plot_data['yhat_upper'], name=f'{confidence_coef*100:.0f}% ç½®ä¿¡åŒºé—´ä¸Šç•Œ',
                             mode='lines', line=dict(width=0), showlegend=False))
    fig.add_trace(go.Scatter(x=plot_data['ds'], y=plot_data['yhat_lower'], name=f'{confidence_coef*100:.0f}% ç½®ä¿¡åŒºé—´',
                             fill='tonexty', mode='lines', line=dict(width=0), fillcolor='rgba(0,100,80,0.2)'))
    # å¼‚å¸¸ç‚¹
    anomalies = plot_data[plot_data['is_anomaly']]
    n_anomalies = plot_data['is_anomaly'].sum()
    title = f"{title}    |    æ£€æµ‹åˆ° {n_anomalies} ä¸ªå¼‚å¸¸ç‚¹ï¼"

    fig.add_trace(go.Scatter(x=anomalies['ds'], y=anomalies['y'], name='å¼‚å¸¸ç‚¹', mode='markers', marker=dict(color='red', size=8)))

    fig.update_layout(
        title=title,
        xaxis_title='æ—¥æœŸ',
        yaxis_title=target,
        legend_title='å›¾ä¾‹',
        hovermode='x unified',
        template='plotly_white',
        xaxis=dict(rangeslider=dict(visible=True), type="date")
    )
    return fig

def get_parameters2(key_prefix="", target_options=None):
    """
    è·å–ç”¨æˆ·è¾“å…¥å‚æ•°ï¼Œå¹¶ä¸ºæ¯ä¸ªç»„ä»¶æŒ‡å®šå”¯ä¸€ key
    """
    col1, col2 = st.columns(2)
    with col1:
        start_date = st.date_input("å¼€å§‹æ—¥æœŸ", value=pd.to_datetime("2024-01-01"), key=f"{key_prefix}_start_date")
        target = st.selectbox("é€‰æ‹©ç›®æ ‡å˜é‡", target_options, key=f"{key_prefix}_target")
    with col2:
        end_date = st.date_input("ç»“æŸæ—¥æœŸ", value=pd.to_datetime("2025-02-28"), key=f"{key_prefix}_end_date")
        analysis_level = st.selectbox("é€‰æ‹©åˆ†æçº§åˆ«", ["SKU", "Category", "Subcategory"], key=f"{key_prefix}_analysis_level")
        
    return start_date, end_date, target, analysis_level

def analyze_data2(data, analysis_level, key_prefix=""):
    """
    æ ¹æ®åˆ†æçº§åˆ«è¿‡æ»¤æ•°æ®ï¼Œå¹¶ç”Ÿæˆæ ‡é¢˜
    """
    if analysis_level == "SKU":
        sku = st.selectbox("é€‰æ‹© SKU", data['sku'].unique(), key=f"{key_prefix}_sku_select")
        category, subcategory = None, None
    elif analysis_level == "Category":
        category = st.selectbox("é€‰æ‹©å“ç±»", data['category'].unique(), key=f"{key_prefix}_category_select")
        sku, subcategory = None, None
    else:
        subcategory = st.selectbox("é€‰æ‹©å­ç±»", data['subcategory'].unique(), key=f"{key_prefix}_subcategory_select")
        sku, category = None, None
    return sku, category, subcategory

def should_refresh_results(cache_key, current_params):
    """æ£€æŸ¥å‚æ•°æ˜¯å¦å‘ç”Ÿå˜åŒ–ï¼Œå†³å®šæ˜¯å¦éœ€è¦åˆ·æ–°ç»“æœ"""
    param_key = f"{cache_key}_params"
    
    # å¦‚æœå‚æ•°æœªä¿å­˜è¿‡ï¼Œæˆ–å‚æ•°å‘ç”Ÿå˜åŒ–ï¼Œåˆ™éœ€è¦åˆ·æ–°
    if param_key not in st.session_state:
        st.session_state[param_key] = current_params
        return True
    
    if st.session_state[param_key] != current_params:
        st.session_state[param_key] = current_params
        return True
    
    return False

def run_analysis_auto(data, sku, category, subcategory, target, start_date, end_date, confidence_coef, cache_key):
    """è‡ªåŠ¨è¿è¡Œåˆ†æå¹¶æ£€æµ‹å‚æ•°å˜åŒ–"""
    # ç”Ÿæˆå½“å‰å‚æ•°ç­¾å
    current_params = {
        'sku': sku, 'category': category, 'subcategory': subcategory,
        'target': target, 
        'start_date': start_date, 'end_date': end_date,
        'confidence_coef': confidence_coef
    }
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°
    if should_refresh_results(cache_key, current_params) or cache_key not in st.session_state:
        with st.spinner("åˆ†æä¸­..."):
            full_data = detect_anomalies_sarimax(data, confidence_coef=confidence_coef)
            st.session_state[cache_key] = {"full_data": full_data}
    
    return st.session_state[cache_key]["full_data"]

def show_result(full_data, title, target, confidence_coef, radio_key):
    """
    å›¾è¡¨åªç”»ä¸€æ¬¡ï¼Œradio åªæ§åˆ¶è¡¨æ ¼
    """
    # ===== 1. ç¼“å­˜å›¾ï¼ˆåªç®—ä¸€æ¬¡ï¼‰=====
    fig_key = f"{radio_key}_fig"
    if fig_key not in st.session_state:
        st.session_state[fig_key] = plot_with_plotly2(
            full_data, title, target, confidence_coef
        )

    st.plotly_chart(st.session_state[fig_key], use_container_width=True)

    # ===== 2. radio åªå½±å“ dataframe =====
    option = st.radio(
        "é€‰æ‹©è¦æ˜¾ç¤ºçš„æ•°æ®ç±»å‹ï¼š",
        ('å…¨éƒ¨æ•°æ®', 'å¼‚å¸¸æ•°æ®', 'æ­£å¸¸æ•°æ®'),
        horizontal=True,
        key=radio_key
    )

    if option == 'å…¨éƒ¨æ•°æ®':
        show_df = full_data
    elif option == 'å¼‚å¸¸æ•°æ®':
        show_df = full_data[full_data['is_anomaly']]
    else:
        show_df = full_data[~full_data['is_anomaly']]

    st.write(f"å½“å‰æ˜¾ç¤º {show_df.shape[0]} æ¡è®°å½•")
    st.dataframe(show_df, use_container_width=True)


st.set_page_config(layout="wide")

st.header('ğŸ“‰Abnormal Analysis')
st.subheader('ğŸ“¥1. Load Data')

# ========= é€‰æ‹©æ•°æ® =========
conn = get_conn()
sources = pd.read_sql("SELECT * FROM data_sources", conn)

with st.form("data_select_form"):
    src = st.selectbox(
        "Select a dataset for modeling",
        sources["name"],
        index=None,
        placeholder="Select a dataset..."
    )
    submitted = st.form_submit_button("â–¶ï¸ Load Data and Analyse")

# ========= è¯»å–æ•°æ® =========
if submitted:
    path = sources.loc[sources["name"] == src, "path"].values[0]
    try:
        df = pd.read_csv(path)
    except Exception as e:
        st.error(f"Failed to load data: {e}")
        st.stop()

    st.session_state.df = df

# ========= å…³é”®é˜²çº¿ï¼ˆä¸æ˜¯å¯é€‰ï¼‰ =========
if (
    'df' not in st.session_state
    or st.session_state.df is None
    or not isinstance(st.session_state.df, pd.DataFrame)
):
    st.info("Please select a dataset and click **Load Data** to continue.")
    st.stop()

# ========= ä»è¿™é‡Œå¼€å§‹ï¼Œdf 100% å®‰å…¨ =========
df = st.session_state.df
st.success(f"Data loaded: {df.shape[0]} rows, {df.shape[1]} columns")


st.subheader('ğŸ”¢2. Variable Profiling & Classification')
st.warning("datetime, level-1, level-2, level-3 ç­‰ç±»å‹åªèƒ½å„é€‰æ‹©ä¸€ä¸ªå­—æ®µï¼Œtarget å¯é€‰æ‹©å¤šä¸ªå­—æ®µï¼")

sample_n = 3
col_widths = [1/2, 2, 1, 1*sample_n, 1, 1, 2]
header_cols = st.columns(col_widths)
header_texts = ["#", "Field Name", "Type", f"Sample ({sample_n})", "Missing %", "Unique", "Classification"]

for col, text in zip(header_cols, header_texts):
    col.markdown(f"**{text}**")

st.markdown('<hr style="margin:1px 0">', unsafe_allow_html=True)  # è¡¨å¤´åˆ†éš”çº¿

prev_config = st.session_state.get('var_config', {})
options = ['exclude', 'datetime', 'target', 'level-1', 'level-2', 'level-3']

config = {}
for idx, c in enumerate(df.columns, 1):
    c_type = df[c].dtype
    c_unique = df[c].nunique()
    cols = st.columns(col_widths)
    # ====== å¡«å……åˆ—å†…å®¹ ======
    with cols[0]:
        st.write(idx)
    with cols[1]:
        st.write(c)
    with cols[2]:
        st.write(str(c_type))
    with cols[3]:
        samples = [str(x) for x in df[c].dropna().unique()[:sample_n]]
        st.markdown(", ".join(samples), unsafe_allow_html=True)
    with cols[4]:
        missing_pct = df[c].isna().mean() * 100
        st.write(f"{missing_pct:.1f}%")
    with cols[5]:
        st.write(c_unique)

    # ===== è‡ªåŠ¨å¡«å……ä¸‹æ‹‰æ¡† =====
    # å…ˆå°è¯•ä»å·²æœ‰é…ç½®å–å€¼
    default_value = prev_config.get(c, None)

    # å¦‚æœæ²¡æœ‰ï¼Œå°±æ ¹æ®ç®€å•è§„åˆ™åˆ¤æ–­
    if default_value is None:
        if 'datetime' in c.lower() or 'date' in c.lower():
            default_value = 'datetime'
        elif 'target' in c.lower():
            default_value = 'target'
        elif c_type =="datetime64[ns]":
            default_value = 'datetime'
        elif c_type =="object":
            default_value = 'level-1'
        elif c_type in ["int64","float64"]:
            default_value = 'target'

    # è®¡ç®—é»˜è®¤ç´¢å¼•
    default_index = options.index(default_value) if default_value in options else 0

    with cols[6]:
        config[c] = st.selectbox(
            '',
            options,
            index=default_index,
            key=f'var_{c}_{idx}',
            label_visibility="collapsed"
        )

    # åˆ†éš”çº¿
    st.markdown('<hr style="margin:1px 0">', unsafe_allow_html=True)

st.session_state.var_config = config

# åˆ é™¤æ— æ•ˆå­—æ®µ
drop_vars = [k for k, v in config.items() if v == 'exclude']
df = df.drop(columns=drop_vars)

st.markdown("")  # ç©ºä¸€è¡Œ

st.subheader('ğŸ“‰3. Abnormal Analysis')
tab1, tab2 = st.tabs(["ğŸ§® ç»Ÿè®¡æ–¹æ³•", "ğŸ§¬ æœºå™¨å­¦ä¹ "])
with tab1:
    target_options = [k for k, v in config.items() if v == 'target']
    dims_options = [k for k, v in config.items() if v in ["level-1", "level-2", "level-3"]]
    start_date, end_date, analysis_level, target, method = get_parameters(key_prefix="sell_in", 
        target_options=target_options, dims_options=dims_options)
    filtered_data, title = analyze_data(df, analysis_level, target, config, start_date, end_date, key_prefix="sell_in")
    # è¿è¡Œåˆ†æ
    if st.button("è¿è¡Œåˆ†æ", key="stat"):
        plot_data = filtered_data.sort_values(by="date")
        fig = plot_with_plotly(plot_data, target, method, f"{title}    |    Target: {target}     |    Method: {method}")
        st.plotly_chart(fig, use_container_width=True)
        st.write("å¼‚å¸¸æ£€æµ‹ç»“æœï¼ˆå«è¾…åŠ©æŒ‡æ ‡ï¼‰ï¼š")
        st.dataframe(plot_data[(plot_data[f'{target}_mz_anomaly'] ==True) | (plot_data[f'{target}_box_anomaly'] ==True)])
with tab2:
    sell_in_target_options = [k for k, v in config.items() if v == 'target']
    start_date, end_date, target, analysis_level = get_parameters2(key_prefix="sell", 
        target_options=sell_in_target_options)
    
    rename_dict = {'å…¥è´¦æ—¥æœŸ': 'date', 'ä¸šåŠ¡å“ç‰Œ': 'brand', 'æ€»å“ç±»': 'category', 'å“ç±»': 'subcategory', 'äº§å“æ ‡å‡†åç§°': 'sku'}
    df.rename(columns=rename_dict, inplace=True)
    sku, category, subcategory = analyze_data2(df, analysis_level, key_prefix="sell_")
    confidence_coef = st.slider("ç½®ä¿¡åº¦", 0.8, 0.99, 0.95, key=f"ML_cc")

    # è¿è¡Œåˆ†æ
    if st.button("è¿è¡Œåˆ†æ", key="ml"):
        try:
            full_data, title = prepare_ts_data(df, sku, category, subcategory, target, start_date, end_date)
            full_data = run_analysis_auto(full_data, sku, category, subcategory, target, start_date, end_date, confidence_coef, cache_key="raw_data")
            show_result(full_data, title, target, confidence_coef, radio_key="sell_in_radio")
        except Exception as e:
            st.error(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            st.info("å¦‚æœçœ‹åˆ°Prophetç›¸å…³é”™è¯¯ï¼Œè¯·å…ˆåœ¨å‘½ä»¤è¡Œä¸­æ‰§è¡Œä¿®å¤æ­¥éª¤ã€‚")
