import streamlit as st
import pandas as pd
import numpy as np
import math
import plotly.express as px
from pathlib import Path
import yaml

from sklearn.preprocessing import StandardScaler, OrdinalEncoder
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import calinski_harabasz_score

from services.db_service import get_conn


# =========================
# Utilities
# =========================
def save_yaml(obj, path):
    with open(path, 'w', encoding='utf-8') as f:
        yaml.dump(obj, f, allow_unicode=True)


# =========================
# Streamlit App
# =========================

st.set_page_config(layout='wide')
st.title('Clustering Analysis')

# Sidebar navigation
step = st.sidebar.radio(
    'Workflow',
    ['Step 0 â€“ Data & Variables', 'Step 1 â€“ Preprocessing', 'Step 2 â€“ Variable Selection',
     'Step 3 â€“ Clustering', 'Step 4 â€“ Evaluation', 'Step 5 â€“ Scoring']
)

# Shared state
if 'df' not in st.session_state:
    st.session_state.df = None
if 'var_config' not in st.session_state:
    st.session_state.var_config = {}
if 'processed_df' not in st.session_state:
    st.session_state.processed_df = None

# =========================
# Step 0: Data & Variable Setup
# =========================

if step == 'Step 0 â€“ Data & Variables':
    st.header('Step 0: Data Import & Variable Classification')
    st.subheader('1. Load Data')

    conn = get_conn()
    sources = pd.read_sql("SELECT * FROM data_sources", conn)

    # ========= æ˜¾å¼é€‰æ‹© + æäº¤ =========
    with st.form("data_select_form"):
        src = st.selectbox(
            "Select a dataset for modeling",
            sources["name"],
            index=None,
            placeholder="Select a dataset..."
        )
        submitted = st.form_submit_button("â–¶ï¸ Load Data and Analyse")

    # ========= åªåœ¨æäº¤æ—¶è¯»æ•°æ® =========
    if submitted:
        path = sources.loc[sources["name"] == src, "path"].values[0]

        try:
            df = pd.read_csv(path)
        except Exception as e:
            st.error(f"Failed to load data: {e}")
            st.stop()

        st.session_state.df = df
        st.session_state.selected_source = src

    # ========= å…³é”®é˜²çº¿ï¼ˆä¸æ˜¯å¯é€‰ï¼‰ =========
    if (
        'df' not in st.session_state
        or st.session_state.df is None
        or not isinstance(st.session_state.df, pd.DataFrame)
    ):
        st.info("Please select a dataset and click **Load Data** to continue.")
        st.stop()

    # ========= ä»è¿™é‡Œå¼€å§‹ï¼Œdf 100% å®‰å…¨ =========
    df = st.session_state.df
    st.success(f"Data loaded: {df.shape[0]} rows, {df.shape[1]} columns")

    # ========= Profiling & Classification =========
    st.subheader('2. Variable Profiling & Classification')

    sample_n = 3
    col_widths = [1/2, 2, 1, 1*sample_n, 1, 1, 2]
    header_cols = st.columns(col_widths)
    header_texts = ["#", "Field Name", "Type", f"Sample ({sample_n})", "Missing %", "Unique", "Classification"]

    for col, text in zip(header_cols, header_texts):
        col.markdown(f"**{text}**")

    st.markdown('<hr style="margin:1px 0">', unsafe_allow_html=True)  # è¡¨å¤´åˆ†éš”çº¿

    prev_config = st.session_state.get('var_config', {})
    options = ['exclude', 'id', 'binary', 'nominal', 'ordinal', 'continuous']

    config = {}
    for idx, c in enumerate(df.columns, 1):
        c_type = df[c].dtype
        c_unique = df[c].nunique()
        cols = st.columns(col_widths)
        # ====== å¡«å……åˆ—å†…å®¹ ======
        with cols[0]:
            st.write(idx)
        with cols[1]:
            st.write(c)
        with cols[2]:
            st.write(str(c_type))
        with cols[3]:
            samples = [str(x) for x in df[c].dropna().unique()[:sample_n]]
            st.markdown(", ".join(samples), unsafe_allow_html=True)
        with cols[4]:
            missing_pct = df[c].isna().mean() * 100
            st.write(f"{missing_pct:.1f}%")
        with cols[5]:
            st.write(c_unique)

        # ===== è‡ªåŠ¨å¡«å……ä¸‹æ‹‰æ¡† =====
        # å…ˆå°è¯•ä»å·²æœ‰é…ç½®å–å€¼
        default_value = prev_config.get(c, None)

        # å¦‚æœæ²¡æœ‰ï¼Œå°±æ ¹æ®ç®€å•è§„åˆ™åˆ¤æ–­
        if default_value is None:
            if '_id' in c.lower():
                default_value = 'id'
            elif c_unique == 2:
                default_value = 'binary'
            elif c_type == 'object':
                default_value = 'nominal'
            elif np.issubdtype(c_type, np.number):
                default_value = 'continuous'
            else:
                default_value = 'exclude'
        # è®¡ç®—é»˜è®¤ç´¢å¼•
        default_index = options.index(default_value) if default_value in options else 0

        with cols[6]:
            config[c] = st.selectbox(
                '',
                options,
                index=default_index,
                key=f'var_{c}_{idx}',
                label_visibility="collapsed"
            )

        # åˆ†éš”çº¿
        st.markdown('<hr style="margin:1px 0">', unsafe_allow_html=True)

    # ä¿å­˜æŒ‰é’®
    if st.button('ğŸ’¾ Save Variable Configuration'):
        st.session_state.var_config = config
        Path('config').mkdir(exist_ok=True)
        save_yaml(config, 'config/segmentation_variables.yaml')
        st.success('Variable configuration saved')

# =========================
# Step 1: Preprocessing
# =========================

if step == 'Step 1 â€“ Preprocessing' and st.session_state.df is not None:
    st.header('Step 1: Preprocessing')
    config = st.session_state.var_config

    col1, col2, col3, col4 = st.columns(4, gap="large")
    with col1:
        misspct = st.slider('Max missing rate', 0.1, 0.9, 0.75, 
            help="Maximum missing percent for a variable. This cannot be greater than .5 if using Hotdeck for imputation;")
    with col2:
        maxpct = st.slider('Max concentration ratio', 0.5, 0.95, 0.9, 
            help="Maximum percent in a single bucket for ordinal and nominal variables;")
    with col3:
        maxcat = st.number_input('Max category', value=25, 
            help="Maximum number of values for ordinal variables. Any variable over this will be re-classified to continuous;")
    with col4:
        maxcorr = st.slider('Max correlation', 0.3, 0.95, 0.7, 
            help="maximum correlation between variables")

    col5, col6 = st.columns(2, gap="large")
    with col5:
        impute_methodO = st.selectbox('Imputation Method for Ordinal variables', ['mean', 'median', 'knn'])
    with col6:
        impute_method = st.selectbox('Imputation Method for Continues variables', ['mean', 'median', 'knn'])
    
    cont_vars = [k for k, v in config.items() if v == 'continuous']
    ord_vars = [k for k, v in config.items() if v == 'ordinal']
    nom_vars = [k for k, v in config.items() if v == 'nominal']
    bin_vars = [k for k, v in config.items() if v == 'binary']
    id_var = [k for k, v in config.items() if v == 'id']

    feat_vars = cont_vars + ord_vars + nom_vars + bin_vars
    df = st.session_state.df[id_var+feat_vars].copy()

    # high missing rate
    drop_vars_mis = [c for c in feat_vars if df[c].isna().mean() > misspct]
    # concentration ratio
    drop_vars_max = [c for c in ord_vars+nom_vars if df[c].value_counts(normalize=True).iloc[0] > maxpct]

    # max category
    ord_vars_cat = [c for c in ord_vars if len(df[c].dropna().unique()) > maxcat]
    # remove vars from ord_vars
    ord_vars = [x for x in ord_vars if x not in ord_vars_cat]
    # add vars into cont_vars
    cont_vars.extend(ord_vars_cat)

    drop_vars = drop_vars_mis + drop_vars_max
    df = df.drop(columns=drop_vars)

    st.write('Dropped variables due to missing', drop_vars)

    # update features
    existing_cols = set(df.columns)
    cont_vars = [c for c in cont_vars if c in existing_cols]
    ord_vars  = [c for c in ord_vars  if c in existing_cols]
    nom_vars  = [c for c in nom_vars  if c in existing_cols]
    bin_vars  = [c for c in bin_vars  if c in existing_cols]

    feat_vars = cont_vars + ord_vars + nom_vars + bin_vars
    
    # ----------------- è®­ç»ƒæŒ‰é’® -----------------
    run_preprocessing = st.button("ğŸš€ Preprocessing")
    
    if run_preprocessing:
        # Nominal recode
        nominal_params = {}
        if nom_vars:
            enc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)
            df[nom_vars] = enc.fit_transform(df[nom_vars].astype(str))
            for i, col in enumerate(nom_vars):
                nominal_params[col] = {'categories': enc.categories_[i].tolist()}

        ordinal_params = {}
        if ord_vars:
            enc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)
            df[ord_vars] = enc.fit_transform(df[ord_vars].astype(str))
            for i, col in enumerate(ord_vars):
                ordinal_params[col] = {'categories': enc.categories_[i].tolist()}

        # Binary recode
        binary_params = {}
        for col in bin_vars:
            true_vals = ['1','y','yes','true']
            df[col] = df[col].apply(lambda x: 1 if str(x).lower() in true_vals else 0)
            binary_params[col] = {'true_values': true_vals}

        # Continues recode and Imputation
        # 1. capping floor
        # 2. Imputation
        continuous_params = {}
        for col in cont_vars:
            # å¡«å……
            if impute_method == 'mean':
                impute_value = df[col].mean()
                df[col].fillna(impute_value, inplace=True)
                fill_value = impute_value
            elif impute_method == 'median':
                impute_value = df[col].median()
                df[col].fillna(impute_value, inplace=True)
                fill_value = impute_value
            # KNNImputer ä¹Ÿå¯ä»¥å•ç‹¬æ‹Ÿåˆï¼Œä½†è¿™é‡Œç®€å•ç¤ºä¾‹ç”¨ median

            # æ ‡å‡†åŒ–
            mean = df[col].mean()
            std = df[col].std()
            df[col] = (df[col] - mean) / std

            # ä¿å­˜å‚æ•°
            continuous_params[col] = {
            'fill_value': float(fill_value), 'mean': float(mean), 'std': float(std)
            }

        # ä¿å­˜åˆ° YAML
        preprocess_pipeline = {
            'continuous': continuous_params,
            'nominal': nominal_params,
            'ordinal': ordinal_params,
            'binary': binary_params
        }

        with open('config/preprocess_pipeline.yaml', 'w', encoding='utf-8') as f:
            yaml.safe_dump(preprocess_pipeline, f, allow_unicode=True)

        st.success("Preprocessing completed and pipeline saved")

        st.session_state.processed_df = df

        st.write('The preprocessed dataset')
        st.dataframe(df.head(), use_container_width=True)

# =========================
# Step 2: Variable Selection
# =========================

if step == 'Step 2 â€“ Variable Selection' and st.session_state.processed_df is not None:
    st.header('Step 2: Variable Selection')
    df = st.session_state.processed_df

    config = st.session_state.var_config
    id_var = [k for k, v in config.items() if v == 'id']

    howmany = st.slider('Number of variables to keep',
        math.ceil(df.shape[1] * 0.5), min(50, df.shape[1]), 
        math.ceil(df.shape[1] * 0.8)
        )

    variances = df.drop(columns=id_var, errors='ignore').var().sort_values(ascending=False)
    selected = variances.head(howmany).index.tolist()

    st.write('Selected variables')
    st.write(selected)

    st.session_state.selected_vars = selected
    df = df[id_var + selected].copy()
    st.session_state.processed_df = df

    st.write('The final dataset used for modeling')
    st.dataframe(df.head(), use_container_width=True)

# =========================
# Step 3: Clustering
# =========================

if step == 'Step 3 â€“ Clustering' and 'selected_vars' in st.session_state:
    st.header('Step 3: K Selection & Clustering')
    df = st.session_state.processed_df
    X = df[st.session_state.selected_vars]

    mink, maxk = st.slider('K range', 2, 10, (3, 8))

    scores = {}
    for k in range(mink, maxk + 1):
        km = KMeans(n_clusters=k, n_init=10, random_state=42)
        labels = km.fit_predict(X)
        scores[k] = calinski_harabasz_score(X, labels)

    score_df = pd.DataFrame.from_dict(scores, orient='index', columns=['CH Score'])
    st.line_chart(score_df)

    best_k = score_df['CH Score'].idxmax()
    st.success(f'Recommended K = {best_k}')

    final_k = st.number_input('Final K', value=int(best_k))
    km = KMeans(n_clusters=final_k, n_init=20, random_state=42)
    df['cluster'] = km.fit_predict(X)

    st.session_state.clustered_df = df
    st.session_state.kmeans = km

# =========================
# Step 4: Evaluation
# =========================

if step == 'Step 4 â€“ Evaluation' and 'clustered_df' in st.session_state:
    st.header('Step 4: Evaluation')
    st.subheader('1. PCA and LDA charts')

    df = st.session_state.clustered_df

    # ---------------- é‡‡æ · ----------------
    max_points = 10000  # æœ€å¤§ç»˜å›¾ç‚¹æ•°
    if df.shape[0] > max_points:
        df_plot = df.sample(n=max_points, random_state=42)
    else:
        df_plot = df.copy()

    X = df_plot[st.session_state.selected_vars]
    y = df_plot['cluster']

    # ---------------- PCA ----------------
    pca = PCA(n_components=2)
    Xp = pca.fit_transform(X)
    pca_df = pd.DataFrame(Xp, columns=['PC1', 'PC2'])
    pca_df['cluster'] = y

    # ---------------- LDA ----------------
    lda = LinearDiscriminantAnalysis(n_components=2)
    Xl = lda.fit_transform(X, y)
    lda_cols = [f'LD{i+1}' for i in range(Xl.shape[1])]
    lda_df = pd.DataFrame(Xl, columns=lda_cols)
    lda_df['cluster'] = y

    # ---------------- å¹¶æ’æ˜¾ç¤ºå›¾è¡¨ ----------------
    col1, col_space, col2 = st.columns([5, 0.5, 5])  # 0.5 ä½œä¸ºç©ºéš™

    with col1:
        st.markdown("PCA Scatter Plot")
        st.scatter_chart(pca_df, x='PC1', y='PC2', color='cluster')

    with col2:
        st.markdown("LDA Scatter Plot")
        st.scatter_chart(
            lda_df,
            x=lda_cols[0],
            y=lda_cols[1] if Xl.shape[1] > 1 else lda_cols[0],
            color='cluster'
        )

    # ========= Variable Summary by Cluster =========
    st.subheader('2. Variable Summary by Cluster')
    selected_vars = st.session_state.selected_vars  # å»ºæ¨¡ä½¿ç”¨çš„ç‰¹å¾åˆ—

    # ---------------- èšç±»å‡å€¼åˆ†æ ----------------
    cluster_means = df.groupby('cluster')[selected_vars].mean().reset_index()
    st.markdown("Cluster Means Table")
    st.dataframe(cluster_means, use_container_width=True)
    
    # ---------------- å¹³è¡Œåæ ‡ç³»å›¾ ----------------
    st.markdown("Parallel Coordinates Plot")
    # Plotly çš„ px.parallel_coordinates éœ€è¦åˆ—æ˜¯æ•°å€¼å‹
    # height_slider = st.slider("Graph Height", 400, 1000, 600)
    fig = px.parallel_coordinates(
        cluster_means,
        color='cluster',
        dimensions=selected_vars,
        color_continuous_scale=px.colors.diverging.Tealrose,
        color_continuous_midpoint=cluster_means['cluster'].mean(),
        labels={c: c for c in selected_vars},  # ä¿ç•™åŸå§‹ç‰¹å¾å
        # height=height_slider,
        height=400
    )
    # è®¾ç½®æ¯ä¸ªç»´åº¦çš„å°æ•°ä½æ•°ï¼ˆä¾‹å¦‚ä¿ç•™2ä½å°æ•°ï¼‰
    for i, col in enumerate(selected_vars):
        fig.data[0].dimensions[i].tickformat = '.2f'  # ä¿ç•™2ä½å°æ•°
    # è°ƒæ•´å­—ä½“å’Œè¾¹è·
    fig.update_layout(
        font=dict(size=18, 
        color="black"  # æ˜ç¡®è®¾ç½®å­—ä½“é¢œè‰²ï¼Œä½†æ˜¯åœ¨ç³»ç»Ÿç™½è‰²èƒŒæ™¯å’Œé»‘è‰²èƒŒæ™¯è§ä¼šä¼šå‡ºç°å¼‚å¸¸
        ), 
        margin=dict(l=80, r=50, t=50, b=50)
    )
    st.plotly_chart(fig, use_container_width=True)

# =========================
# Step 5: Scoring
# =========================

if step == 'Step 5 â€“ Scoring' and 'kmeans' in st.session_state:
    st.header('Step 5: Scoring New Dataset')

    conn = get_conn()
    sources = pd.read_sql("SELECT * FROM data_sources", conn)

    # ========= æ˜¾å¼é€‰æ‹© + æäº¤ =========
    with st.form("scoring_form"):
        src = st.selectbox(
            "Select a dataset for prediction",
            sources["name"],
            index=None,
            placeholder="Select a dataset..."
        )
        submitted = st.form_submit_button("â–¶ï¸ Run")

    # ========= åªåœ¨æäº¤æ—¶è¿è¡Œè¯„åˆ† =========
    if submitted and src is not None:
        path = sources.loc[sources["name"] == src, "path"].values[0]

        try:
            config = st.session_state.var_config
            id_var = [k for k, v in config.items() if v == 'id']
            selected = st.session_state.selected_vars
            new_df = pd.read_csv(path, usecols=id_var + selected)
            st.write('Sample of new dataset')
            st.dataframe(new_df.head(), use_container_width=True)
        except Exception as e:
            st.error(f"Failed to load data: {e}")
            st.stop()

        # åŠ è½½é¢„å¤„ç† pipeline
        try:
            with open('config/preprocess_pipeline.yaml', 'r', encoding='utf-8') as f:
                pipeline = yaml.safe_load(f)
        except Exception as e:
            st.error(f"Failed to load preprocessing pipeline: {e}")
            st.stop()

        # ------------------- åä¹‰å˜é‡ -------------------
        for col, params in pipeline.get('nominal', {}).items():
            if col in new_df.columns:
                enc = OrdinalEncoder(
                    categories=[params['categories']],
                    handle_unknown='use_encoded_value',
                    unknown_value=-1
                )
                new_df[[col]] = enc.fit_transform(new_df[[col]].astype(str))

        # ------------------- æœ‰åºå˜é‡ -------------------
        for col, params in pipeline.get('ordinal', {}).items():
            if col in new_df.columns:
                enc = OrdinalEncoder(
                    categories=[params['categories']],
                    handle_unknown='use_encoded_value',
                    unknown_value=-1
                )
                new_df[[col]] = enc.fit_transform(new_df[[col]].astype(str))

        # ------------------- äºŒå€¼å˜é‡ -------------------
        for col, params in pipeline.get('binary', {}).items():
            if col in new_df.columns:
                true_vals = params['true_values']
                new_df[col] = new_df[col].apply(lambda x: 1 if str(x).lower() in true_vals else 0)

        # ------------------- è¿ç»­å˜é‡ -------------------
        for col, params in pipeline.get('continuous', {}).items():
            if col in new_df.columns:
                new_df[col].fillna(params['fill_value'], inplace=True)
                new_df[col] = (new_df[col] - params['mean']) / params['std']

        # ç‰¹å¾å¯¹é½ï¼ˆå¿…é¡»ä¸å»ºæ¨¡æ—¶é¡ºåºä¸€è‡´ï¼‰
        Xnew = new_df[st.session_state.selected_vars]
        labels = st.session_state.kmeans.predict(Xnew)
        new_df['cluster'] = labels

        # ä¿å­˜ç»“æœåˆ° session state
        st.session_state.scoring_result = new_df.copy()

    # ========= å…³é”®é˜²çº¿ï¼šæ£€æŸ¥æ˜¯å¦æœ‰ç»“æœ =========
    if 'scoring_result' not in st.session_state:
        st.info("Please select a dataset and click **Run** to score new data.")
        st.stop()

    # ========= ä»è¿™é‡Œå¼€å§‹ï¼Œç»“æœ 100% å®‰å…¨ =========
    result_df = st.session_state.scoring_result
    st.success(f"Scoring completed: {result_df.shape[0]} rows classified into clusters.")

    st.write("Sample of scoring result:")
    st.dataframe(result_df.head(), use_container_width=True)
    st.download_button(
                    "ğŸ’¾ ä¸‹è½½èšç±»ç»“æœå®Œæ•´æ˜ç»†æ•°æ®",
                    data=result_df.to_csv().encode('utf-8'),
                    file_name="Ignite platform-Segmentation Results.csv",
                    mime="csv")
